---
title: 'Graph 3: PCA Correlations '
author: ''
date: '2018-06-12'
slug: graph-4-tba
categories: []
tags: []
description: Desc
hacker_news_id: ''
lobsters_id: ''
meta_img: /images/image.jpg
---



<div id="preliminaries" class="section level2">
<h2>Preliminaries</h2>
</div>
<div id="data" class="section level2">
<h2>Data</h2>
<p>All the graphs of this series are generated from data concerning frequency with which words from assorted dictionaries appear in original reddit posts from various subreddits.</p>
<p>Because <code>alldicts</code> is of such high dimensionality– 65 is a lot of features to keep track of– I ran principle component analysis (PCA) on the data to reduce the dimensionality to 21 features, while still explaining 90% of the variance within the dataset. The draw back of this approach is that the principle components can be difficult to interpret. Thus, the plots below explore the relationship/correlation between each of these the 21 principle components a the original 65 features.</p>
<pre class="r"><code># original data
nfreqs &lt;- read_csv(here(&#39;static&#39;,&#39;data&#39;,&#39;redditProject&#39;,&#39;normal_subset_freqs.csv&#39;))</code></pre>
<pre><code>## Parsed with column specification:
## cols(
##   .default = col_double(),
##   subreddit = col_character()
## )</code></pre>
<pre><code>## See spec(...) for full column specifications.</code></pre>
<pre class="r"><code># data in PCA space
redfreqs &lt;- read_csv(here(&#39;static&#39;,&#39;data&#39;,&#39;redditProject&#39;,&#39;reduced_normal_subset_freqs&#39;))</code></pre>
<pre><code>## Warning: Missing column names filled in: &#39;X1&#39; [1]</code></pre>
<pre><code>## Parsed with column specification:
## cols(
##   .default = col_double(),
##   X1 = col_integer()
## )
## See spec(...) for full column specifications.</code></pre>
<pre class="r"><code>#Orininal dimensionality, # of principle components used
od_num=65
pc_num=21

# Calculate correlation
correlations &lt;- cor(nfreqs[c(2:(od_num+1))],redfreqs[c(2:(pc_num+1))])

#Tidyify
corlong &lt;- melt(correlations ) %&gt;% 
   setNames(c(&#39;orig&#39;, &#39;red_dim&#39;, &#39;correlation&#39;))</code></pre>
<div id="audience" class="section level3">
<h3>Audience</h3>
<p>In its current form, audience for this project is almost entirely myself and the professors I’m trying to convince to help me with this project. Eventually I hope to research may be of interest to a larger group, including:</p>
<ol style="list-style-type: decimal">
<li>Linguists interested in this method of textual analysis and the language of internet forums</li>
<li>Computer sciencists interested in the distributed computing or machine learning that went into this project</li>
<li>Social scientists studying any of the many social groups involved on reddit.</li>
</ol>
</div>
</div>
<div id="plot-type-tiled-correlations" class="section level2">
<h2>Plot Type: Tiled Correlations</h2>
<div id="how-to-read-it-and-what-to-look-for" class="section level3">
<h3>How to read it and what to look for</h3>
<p>This graph is essentially a heat map of the correlations between the features. Each of the axes in this chart represents a set of features, and the values in the chart (in this case correlation) are represented by colored tiles.</p>
</div>
<div id="presentation-tips" class="section level3">
<h3>Presentation Tips</h3>
<p>For high dimensionality datasets, with a large number of features on one or both axes, this chart can be hard to interpret as it can be diffilcut to see which tiles represent which features. As such, this graph is best for sets of seven for fewer features, so that tiles are relatively close to their labels and columns are parseable. Alternatively, some method of highlighting important tiles and their matching labels could help address this.</p>
<p>Additionally, because values are represented by color in this graph, which is not particularly effective at for communicating numerical specifics, this chart should not be used when actual values are important. It is more effective when the goal is to highlight extreme values or patterns.</p>
<p>To represent correlation, a diverging color scheme with high saturation at the extremes and low saturation in the center is appropriate. This way color represents the binary value of sign and saturation shows distance of a value from zero.</p>
</div>
<div id="variations-and-alternatives" class="section level3">
<h3>Variations and Alternatives</h3>
<p>A more common use of this graph is to show, within a a dataset, covariance between features. In that version, the features on each axis identical would be the same and the values along the axis.</p>
<p>would be 1 because the correlation would be complete because they are actually the same values. This plot is following that same theme, and while the same information is represented on both axes, it is after the PCA. The axes tell what features we’re looking at, and the tiles on the map are colored according to their values. In this case, they are colored by the magnitude of correlation.</p>
<p>It has two discrete axes.</p>
<p>Slightly more common to illustrate covariation within a data frame. Also used to represent successful and failed categorization (true positives and true negatives).</p>
</div>
</div>
<div id="plot-description" class="section level2">
<h2>Plot Description</h2>
<p>Principal components on the x-axis and features one the y-axis, with tiles colored by correlation. The decision to have original on y and reduced on x is arbitrary. Then I kept the smaller number of values on the x-axis mostly for legibility of the labels. My color scheme ranges from dark purple to dark green with white in the center in order to make values that are far from 0 pop out. I was unable to find a way to highlight specific rows and columns, but that would be a useful addition to the plot.</p>
<div id="representaton" class="section level3">
<h3>Representaton</h3>
<pre class="r"><code>ggplot(corlong, aes(x=red_dim, y=orig, fill=correlation))+
  geom_tile()+ 
  theme(axis.text.x = element_text(angle = 60, hjust = 1))+
  scale_fill_distiller(palette = &quot;PRGn&quot;)</code></pre>
<p><img src="/post/2018-06-11-graph-3-tba_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<pre class="r"><code>  #scale_fill_viridis(option=&#39;E&#39;)</code></pre>
</div>
<div id="methods" class="section level3">
<h3>Methods</h3>
<pre class="r"><code>covariances &lt;- read_csv(here(&#39;static&#39;,&#39;data&#39;,&#39;redditProject&#39;,&#39;subset_covariance.csv&#39;))</code></pre>
<pre><code>## Warning: Missing column names filled in: &#39;X1&#39; [1]</code></pre>
<pre><code>## Parsed with column specification:
## cols(
##   .default = col_double(),
##   X1 = col_character()
## )</code></pre>
<pre><code>## See spec(...) for full column specifications.</code></pre>
<pre class="r"><code>covlong &lt;- gather(covariances, key=newDimension, value=covariance, -X1)

ggplot(covlong, aes(x=newDimension, y=X1, fill=covariance))+
  geom_tile()+
  theme(axis.text.x = element_text(angle = 60, hjust = 1))</code></pre>
<p><img src="/post/2018-06-11-graph-3-tba_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
</div>
</div>
